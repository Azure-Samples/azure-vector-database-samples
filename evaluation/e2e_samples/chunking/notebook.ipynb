{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking E2E Sample\n",
    "\n",
    "This notebook explores some of the evaluation metrics that can be used to measure the affectiveness of the retrieval of chunked content. This notebook is a complete end-to-end solution that engests pre-embedded content into Azure AI search and evaluates the results of search on a QA pair dataset.\n",
    "\n",
    "This notebook plans to outline two evaluation metrics:\n",
    "\n",
    "- RougeL: ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence) is a metric used in the field of Natural Language Processing for evaluating the quality of summaries by comparing them to reference summaries. \n",
    "- In-top: Checks if the expected source is contained in any of the chunks. If the source is found in atleast 1 of the retrieved chunks, the in-top is 1. If the source is not found in the retrieved chunks, the in-top is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.models import RawVectorQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchIndex,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SearchField,\n",
    "    SemanticSettings,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,\n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from dotenv import dotenv_values\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from openai.resources import Embeddings\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import time\n",
    "\n",
    "_rouge_scorer = rouge_scorer.RougeScorer(rouge_types=[\"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up Configs\n",
    "\n",
    "This notebook expects several environment variables (which are outlined below). The environment variables are used to connect to **Azure AI Search** and **Azure OpenAI**.\n",
    "\n",
    "The required environment variables are the following:\n",
    "- **AIS_ENDPOINT** [Required]: The Azure AI Search endpoint.\n",
    "- **AIS_API_VERSION** [Required]: The Azure AI Search version (ex: `2023-11-01`).\n",
    "- **AIS_KEY** [Required]: The Azure AI Search key.\n",
    "- **AOAI_ENDPOINT** [Required]: The Azure OpenAI endpoint.\n",
    "- **AOAI_API_VERSION** [Required]: The Azure OpenAI version (ex: `2023-09-01-preview`).\n",
    "- **AZURE_OPENAI_KEY** [Required]: The Azure OpenAI key.\n",
    "- **AOAI_EMBEDDING_DEPLOYED_MODEL** [Required]: The Azure OpenAI embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_values = dotenv_values()\n",
    "\n",
    "class Constants(object):\n",
    "    INDEX_NAME = \"evaluation-index\"\n",
    "    QA_DATASET_PATH = \"../../../code_samples/data/e2e_samples/chunking/ground_truth/qa_dataset.csv\"\n",
    "    EMBEDDING_DATA_PATH = \"../../../code_samples/data/e2e_samples/chunking/embeddings.json\"\n",
    "    OUTPUT_FILE_PATH = \"./output/chunk_results.json\"\n",
    "\n",
    "class AzureAiSearchConfig(object):\n",
    "    _API_ENDPOINT_KEY = \"AIS_ENDPOINT\"\n",
    "    _API_VERSION_KEY = \"AIS_API_VERSION\"\n",
    "    _API_KEY_KEY = \"AIS_KEY\"\n",
    "\n",
    "    _REQUIRED_KEYS = [\n",
    "        _API_ENDPOINT_KEY,\n",
    "        _API_VERSION_KEY,\n",
    "        _API_KEY_KEY\n",
    "    ]\n",
    "\n",
    "    api_endpoint: str\n",
    "    api_version: str\n",
    "    api_key: str\n",
    "\n",
    "    def __init__(self, env_values: dict[str, any]):\n",
    "        _api_endpoint = env_values.get(AzureAiSearchConfig._API_ENDPOINT_KEY)\n",
    "        _api_version = env_values.get(AzureAiSearchConfig._API_VERSION_KEY)\n",
    "        _api_key = env_values.get(AzureAiSearchConfig._API_KEY_KEY)\n",
    "\n",
    "        if (not _api_endpoint or not _api_version or not _api_key):\n",
    "            raise ValueError(f\"The following environment variables are required: {', '.join(AzureAiSearchConfig._REQUIRED_KEYS)}\")\n",
    "\n",
    "        self.api_endpoint = _api_endpoint\n",
    "        self.api_version = _api_version\n",
    "        self.api_key = _api_key\n",
    "\n",
    "class AzureOpenAiConfig(object):\n",
    "    _API_ENDPOINT_KEY = \"AOAI_ENDPOINT\"\n",
    "    _API_VERSION_KEY = \"AOAI_API_VERSION\"\n",
    "    _API_KEY_KEY = \"AZURE_OPENAI_KEY\"\n",
    "    _DEPLOYMENT_MODEL_KEY = \"AOAI_EMBEDDING_DEPLOYED_MODEL\"\n",
    "\n",
    "    _REQUIRED_KEYS = [\n",
    "        _API_ENDPOINT_KEY,\n",
    "        _API_VERSION_KEY,\n",
    "        _API_KEY_KEY,\n",
    "        _DEPLOYMENT_MODEL_KEY\n",
    "    ]\n",
    "\n",
    "    api_endpoint: str\n",
    "    api_version: str\n",
    "    api_key: str\n",
    "    deployment_model: str\n",
    "\n",
    "    def __init__(self, env_values: dict[str, any]):\n",
    "        _api_endpoint = env_values.get(AzureOpenAiConfig._API_ENDPOINT_KEY)\n",
    "        _api_version = env_values.get(AzureOpenAiConfig._API_VERSION_KEY)\n",
    "        _api_key = env_values.get(AzureOpenAiConfig._API_KEY_KEY)\n",
    "        _deployment_model = env_values.get(AzureOpenAiConfig._DEPLOYMENT_MODEL_KEY)\n",
    "\n",
    "        if (not _api_endpoint or not _api_version or not _api_key):\n",
    "            raise ValueError(f\"The following environment variables are required: {', '.join(AzureOpenAiConfig._REQUIRED_KEYS)}\")\n",
    "\n",
    "        self.api_endpoint = _api_endpoint\n",
    "        self.api_version = _api_version\n",
    "        self.api_key = _api_key\n",
    "        self.deployment_model = _deployment_model\n",
    "\n",
    "\n",
    "azure_openai_config = AzureOpenAiConfig(env_values)\n",
    "azure_ai_search_config = AzureAiSearchConfig(env_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure search fields\n",
    "_ID_FIELD = \"id\"\n",
    "_CONTENT_FIELD = \"chunk_content\"\n",
    "_VECTOR_FIELD = \"chunk_content_vector\"\n",
    "_METADATA_FIELD = \"metadata\"\n",
    "_SOURCE_METADATA_FIELD = \"source\"\n",
    "\n",
    "_SELECT_FIELDS = [\n",
    "    _ID_FIELD,\n",
    "    _CONTENT_FIELD,\n",
    "    _VECTOR_FIELD,\n",
    "    _METADATA_FIELD,\n",
    "]\n",
    "\n",
    "# index config fields\n",
    "_DEFAULT_SEMANTIC_CONFIG_NAME = \"default\"\n",
    "_HNSW_ALGORITHM_CONFIG_NAME = \"hnsw_config\"\n",
    "_VECTOR_SEARCH_PROFILE_NAME = \"hnsw_profile\"\n",
    "\n",
    "# hnsw configs\n",
    "_M = 4\n",
    "_EF_CONSTRUCTION = 400\n",
    "_EF_SEARCH = 500\n",
    "_METRIC = \"cosine\"\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    id: str\n",
    "    content: str\n",
    "    score: float\n",
    "    source: str\n",
    "\n",
    "\n",
    "class _Embedding(object):\n",
    "    _embeddings: Embeddings\n",
    "    _embedding_model: str\n",
    "\n",
    "    def __init__(self, embeddings: Embeddings, embedding_model: str):\n",
    "        self._embeddings = embeddings\n",
    "        self._embedding_model = embedding_model\n",
    "\n",
    "    def embed(self, text: str):\n",
    "        return self._embeddings.create(input = [text], model=self._embedding_model).data[0].embedding\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureOpenAiConfig):\n",
    "        client = AzureOpenAI(\n",
    "            api_key = config.api_key,  \n",
    "            api_version = config.api_version,\n",
    "            azure_endpoint = config.api_endpoint\n",
    "        )\n",
    "        embeddings = client.embeddings\n",
    "        return cls(embeddings, config.deployment_model)\n",
    "\n",
    "\n",
    "class _SearchClient(object):\n",
    "    _BATCH_SIZE = 1000\n",
    "\n",
    "    search_client: SearchClient\n",
    "\n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self._search_client = search_client\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        embedding_vector: list[float],\n",
    "        k: int = 3,\n",
    "        vector_field: str = _VECTOR_FIELD,\n",
    "        select: list[str] = _SELECT_FIELDS,\n",
    "    ):\n",
    "        vector_query = RawVectorQuery(vector=embedding_vector, k=k, fields=vector_field)\n",
    "        return self._search_client.search(  \n",
    "            search_text=query,  \n",
    "            vector_queries= [vector_query],\n",
    "            select=select,\n",
    "        )\n",
    "\n",
    "    def upload(\n",
    "        self,\n",
    "        documents: list[dict]\n",
    "    ):\n",
    "        batch_size = _SearchClient._BATCH_SIZE\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            self._search_client.upload_documents(documents=batch)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureAiSearchConfig, index_name: str = Constants.INDEX_NAME):\n",
    "        search_client = SearchClient(config.api_endpoint, index_name, AzureKeyCredential(config.api_key))\n",
    "        return cls(search_client)\n",
    "\n",
    "class _SearchIndexClient(object):\n",
    "    _search_index_client: SearchIndexClient\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_index_client: SearchIndexClient\n",
    "    ):\n",
    "        self._search_index_client = search_index_client\n",
    "\n",
    "    def _get_index_client(self, index_name: str):\n",
    "        return self._search_index_client.get_index(index_name)\n",
    "\n",
    "    def _create_index_client(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        with_semantic_search: bool = False\n",
    "    ):\n",
    "        fields = [\n",
    "            SimpleField(name=_ID_FIELD, type=SearchFieldDataType.String, key=True),\n",
    "            SearchableField(name=_CONTENT_FIELD, type=SearchFieldDataType.String),\n",
    "            SearchField(name=_VECTOR_FIELD, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile=_VECTOR_SEARCH_PROFILE_NAME),\n",
    "            SearchableField(name=_METADATA_FIELD, type=SearchFieldDataType.String)\n",
    "        ]\n",
    "\n",
    "        vector_search = VectorSearch(\n",
    "            algorithms=[\n",
    "                HnswVectorSearchAlgorithmConfiguration(\n",
    "                    name=_HNSW_ALGORITHM_CONFIG_NAME,\n",
    "                    kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                    parameters=HnswParameters(\n",
    "                        m=_M,\n",
    "                        ef_construction=_EF_CONSTRUCTION,\n",
    "                        ef_search=_EF_SEARCH,\n",
    "                        metric=_METRIC\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            profiles=[\n",
    "                VectorSearchProfile(\n",
    "                    name=_VECTOR_SEARCH_PROFILE_NAME,\n",
    "                    algorithm=_HNSW_ALGORITHM_CONFIG_NAME\n",
    "                )\n",
    "            ]  \n",
    "        )\n",
    "\n",
    "        semantic_settings: SemanticSettings | None = None\n",
    "        if with_semantic_search:\n",
    "            semantic_settings = SemanticSettings(\n",
    "                configuration=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=_DEFAULT_SEMANTIC_CONFIG_NAME,\n",
    "                        prioritized_fields=PrioritizedFields(\n",
    "                            prioritized_content_field=[\n",
    "                                SemanticField(field_name=_CONTENT_FIELD),\n",
    "                                SemanticField(field_name=_METADATA_FIELD)\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=fields,\n",
    "            vector_search=vector_search,\n",
    "            semantic_settings=semantic_settings)\n",
    "        return self._search_index_client.create_index(index)\n",
    "    \n",
    "    def get_or_create_index_client(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        with_semantic_search: bool = False\n",
    "    ):\n",
    "        try:\n",
    "            return self._get_index_client(index_name)\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"Index {index_name} does not exist... Creating a new index.\")\n",
    "            return self._create_index_client(index_name, with_semantic_search)\n",
    "\n",
    "    def delete_index(self, index_name: str):\n",
    "        self._search_index_client.delete_index(index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureAiSearchConfig):\n",
    "        search_index_client = SearchIndexClient(config.api_endpoint, AzureKeyCredential(config.api_key))\n",
    "        return cls(search_index_client)\n",
    "\n",
    "\n",
    "class GroundTruthDatasetLoader(object):\n",
    "    # df keys\n",
    "    _QUESTION_KEY = \"question\"\n",
    "    _ANSWER_KEY = \"answer\"\n",
    "    _SOURCE_KEY = \"source\"\n",
    "    _SEARCH_RESULT_KEY = \"search_result\"\n",
    "\n",
    "    _df: pd.DataFrame\n",
    "\n",
    "    def __init__(self, path: str = Constants.QA_DATASET_PATH):\n",
    "        self._df = pd.read_csv(path)\n",
    "\n",
    "    def load_rows(self):\n",
    "        for i, row in self._df.iterrows():\n",
    "            yield i, row\n",
    "\n",
    "    def get_question_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._QUESTION_KEY]\n",
    "    \n",
    "    def get_answer_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._ANSWER_KEY]\n",
    "    \n",
    "    def get_source_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._SOURCE_KEY]\n",
    "    \n",
    "    def get_seach_answers_from_row(self, row: pd.Series) -> list[SearchResult]:\n",
    "        search_results = json.loads(row[GroundTruthDatasetLoader._SEARCH_RESULT_KEY])\n",
    "        return [SearchResult(**search_result) for search_result in search_results]\n",
    "\n",
    "    def set_search_answers_to_row(self, index: int, search_response: list[SearchResult]):\n",
    "        value = json.dumps([dataclasses.asdict(elem) for elem in search_response])\n",
    "        self.set_key(index, GroundTruthDatasetLoader._SEARCH_RESULT_KEY, value)\n",
    "\n",
    "    def set_key(self, index: int, key: str, value: any):\n",
    "        self._df.at[index, key] = value\n",
    "\n",
    "    def save(self, path: str):\n",
    "        self._df.to_json(path, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the Orchestrator\n",
    "\n",
    "The following cell runs the end-to-end solution to perform teh following steps:\n",
    "1. Clears the index if exists.\n",
    "1. Ingest the embedded chunks into Azure AI Search.\n",
    "1. Performs Search on the questions in the QA dataset.\n",
    "1. Evaluates the results of search against the QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting index evaluation-index...\n",
      "Creating index and uploading data...\n",
      "Index evaluation-index does not exist... Creating a new index.\n",
      "Performing search...\n",
      "Evaluting chunks...\n"
     ]
    }
   ],
   "source": [
    "class ExperimentOrchestrator(object):\n",
    "    _IN_TOP_KEY = \"in_top\"\n",
    "    _ROUGE_L_KEY = \"rougeL\"\n",
    "\n",
    "    _dataset: list[dict]\n",
    "    _index_name: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_index_client: _SearchIndexClient,\n",
    "        search_client: _SearchClient,\n",
    "        embedding: _Embedding,\n",
    "        ground_truth_dataset_loader: GroundTruthDatasetLoader,\n",
    "        index_name: str = Constants.INDEX_NAME,\n",
    "        embedding_path: str = Constants.EMBEDDING_DATA_PATH\n",
    "    ):\n",
    "        # delete the index\n",
    "        print(f\"Deleting index {index_name}...\")\n",
    "        search_index_client.delete_index(index_name)\n",
    "        time.sleep(2)\n",
    "\n",
    "        self._search_index_client = search_index_client\n",
    "        self._search_client = search_client\n",
    "        self._embedding = embedding\n",
    "        self._ground_truth_dataset_loader = ground_truth_dataset_loader\n",
    "        self._index_name = index_name\n",
    "        self._embedding_data = self._read_json_dataset(embedding_path)\n",
    "\n",
    "    def _read_json_dataset(self, path: str):\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def run(self, output_file_path: str):\n",
    "        # create the index if not exists and uploading the data\n",
    "        print(f\"Creating index and uploading data...\")\n",
    "        self._search_index_client.get_or_create_index_client(self._index_name)\n",
    "        self._search_client.upload(self._embedding_data)\n",
    "        time.sleep(2) # adding wait as documents may need to settle\n",
    "\n",
    "        print(\"Performing search...\")\n",
    "        for index, row in self._ground_truth_dataset_loader.load_rows():\n",
    "            question = self._ground_truth_dataset_loader.get_question_from_row(row)\n",
    "            embedding = self._embedding.embed(question)\n",
    "            search_result_paged = self._search_client.search(question, embedding)\n",
    "\n",
    "            search_results_final: list[SearchResult] = []\n",
    "            for item in search_result_paged:\n",
    "                del item[_VECTOR_FIELD]\n",
    "                source = json.loads(item[_METADATA_FIELD])[_SOURCE_METADATA_FIELD]\n",
    "                search_result = SearchResult(\n",
    "                    id=item[_ID_FIELD],\n",
    "                    content=item[_CONTENT_FIELD],\n",
    "                    score=item[\"@search.score\"],\n",
    "                    source=source\n",
    "                )\n",
    "                search_results_final.append(search_result)\n",
    "            self._ground_truth_dataset_loader.set_search_answers_to_row(index, search_results_final)\n",
    "\n",
    "        print(\"Evaluting chunks...\")\n",
    "        for index, row in self._ground_truth_dataset_loader.load_rows():\n",
    "            # calculate in top init\n",
    "            expected_answer = self._ground_truth_dataset_loader.get_answer_from_row(row)\n",
    "            expected_source = self._ground_truth_dataset_loader.get_source_from_row(row)\n",
    "            search_results = self._ground_truth_dataset_loader.get_seach_answers_from_row(row)\n",
    "            search_sources = [search_result.source for search_result in search_results]\n",
    "\n",
    "            contains_source = 1 if expected_source in search_sources else 0\n",
    "            self._ground_truth_dataset_loader.set_key(index, ExperimentOrchestrator._IN_TOP_KEY, contains_source)\n",
    "\n",
    "            # rouge L metric\n",
    "            max_rougeL_recall = -1\n",
    "            for search_result in search_results:\n",
    "                search_content = search_result.content\n",
    "                rougeL_recall = _rouge_scorer.score(target=expected_answer, prediction=search_content)[\"rougeL\"].recall\n",
    "                max_rougeL_recall = rougeL_recall if rougeL_recall > max_rougeL_recall else max_rougeL_recall\n",
    "            self._ground_truth_dataset_loader.set_key(index, ExperimentOrchestrator._ROUGE_L_KEY, max_rougeL_recall)\n",
    "\n",
    "        self._ground_truth_dataset_loader.save(output_file_path)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls,\n",
    "        azure_ai_search_config: AzureAiSearchConfig,\n",
    "        azure_openai_config: AzureOpenAiConfig,\n",
    "        index_name: str,\n",
    "        embedding_path: str\n",
    "    ):\n",
    "        search_client_index = _SearchIndexClient.from_config(azure_ai_search_config)\n",
    "        search_client = _SearchClient.from_config(azure_ai_search_config, index_name)\n",
    "        embedding = _Embedding.from_config(azure_openai_config)\n",
    "        \n",
    "        return cls(search_client_index, search_client, embedding, GroundTruthDatasetLoader(), index_name, embedding_path)\n",
    "\n",
    "orchestrator = ExperimentOrchestrator.from_config(\n",
    "    azure_ai_search_config,\n",
    "    azure_openai_config,\n",
    "    Constants.INDEX_NAME,\n",
    "    Constants.EMBEDDING_DATA_PATH\n",
    ")\n",
    "orchestrator.run(Constants.OUTPUT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying the Average Evaluation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rougeL: 0.953963014495\n",
      "Average in top: 1.0\n"
     ]
    }
   ],
   "source": [
    "# reading the output file and getting the average rougeL and in top metric\n",
    "df = pd.read_json(Constants.OUTPUT_FILE_PATH)\n",
    "print(f\"Average rougeL: {df[ExperimentOrchestrator._ROUGE_L_KEY].mean()}\")\n",
    "print(f\"Average in top: {df[ExperimentOrchestrator._IN_TOP_KEY].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
