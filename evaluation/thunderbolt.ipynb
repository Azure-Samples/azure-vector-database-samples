{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: trectools in /home/vscode/.local/lib/python3.9/site-packages (0.0.49)\n",
      "Requirement already satisfied: evaluate in /home/vscode/.local/lib/python3.9/site-packages (0.4.1)\n",
      "Requirement already satisfied: openai in /home/vscode/.local/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: unstructured==0.11.8 in /home/vscode/.local/lib/python3.9/site-packages (0.11.8)\n",
      "Requirement already satisfied: pandas==2.1.4 in /home/vscode/.local/lib/python3.9/site-packages (2.1.4)\n",
      "Requirement already satisfied: azure-search-documents==11.4.0b11 in /home/vscode/.local/lib/python3.9/site-packages (11.4.0b11)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /home/vscode/.local/lib/python3.9/site-packages (1.0.0)\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: chardet in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (5.2.0)\n",
      "Requirement already satisfied: filetype in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (1.2.0)\n",
      "Requirement already satisfied: python-magic in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (0.4.27)\n",
      "Requirement already satisfied: lxml in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (5.0.1)\n",
      "Requirement already satisfied: nltk in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (0.9.0)\n",
      "Requirement already satisfied: requests in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (4.12.2)\n",
      "Requirement already satisfied: emoji in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (2.9.0)\n",
      "Requirement already satisfied: dataclasses-json in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (0.6.3)\n",
      "Requirement already satisfied: python-iso639 in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (2024.1.2)\n",
      "Requirement already satisfied: langdetect in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (1.0.9)\n",
      "Requirement already satisfied: numpy in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (1.26.3)\n",
      "Requirement already satisfied: rapidfuzz in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (3.6.1)\n",
      "Requirement already satisfied: backoff in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (4.9.0)\n",
      "Requirement already satisfied: unstructured-client in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (0.15.2)\n",
      "Requirement already satisfied: wrapt in /home/vscode/.local/lib/python3.9/site-packages (from unstructured==0.11.8) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vscode/.local/lib/python3.9/site-packages (from pandas==2.1.4) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas==2.1.4) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/vscode/.local/lib/python3.9/site-packages (from pandas==2.1.4) (2023.4)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.24.0 in /home/vscode/.local/lib/python3.9/site-packages (from azure-search-documents==11.4.0b11) (1.29.6)\n",
      "Requirement already satisfied: azure-common~=1.1 in /home/vscode/.local/lib/python3.9/site-packages (from azure-search-documents==11.4.0b11) (1.1.28)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /home/vscode/.local/lib/python3.9/site-packages (from azure-search-documents==11.4.0b11) (0.6.1)\n",
      "Requirement already satisfied: scikit-learn>=0.15 in /home/vscode/.local/lib/python3.9/site-packages (from trectools) (1.3.2)\n",
      "Requirement already satisfied: scipy>=0.10.0 in /home/vscode/.local/lib/python3.9/site-packages (from trectools) (1.11.4)\n",
      "Requirement already satisfied: sarge>=0.1.1 in /home/vscode/.local/lib/python3.9/site-packages (from trectools) (0.1.7.post1)\n",
      "Requirement already satisfied: bs4>=0.0.0.1 in /home/vscode/.local/lib/python3.9/site-packages (from trectools) (0.0.1)\n",
      "Requirement already satisfied: matplotlib>=1.5 in /home/vscode/.local/lib/python3.9/site-packages (from trectools) (3.8.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: dill in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /home/vscode/.local/lib/python3.9/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (0.20.2)\n",
      "Requirement already satisfied: packaging in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /home/vscode/.local/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/vscode/.local/lib/python3.9/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/vscode/.local/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/vscode/.local/lib/python3.9/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /home/vscode/.local/lib/python3.9/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /home/vscode/.local/lib/python3.9/site-packages (from openai) (1.3.0)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/vscode/.local/lib/python3.9/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/vscode/.local/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/vscode/.local/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/vscode/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /home/vscode/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/vscode/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: certifi in /home/vscode/.local/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/vscode/.local/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/vscode/.local/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (4.47.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/vscode/.local/lib/python3.9/site-packages (from matplotlib>=1.5->trectools) (6.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/vscode/.local/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/vscode/.local/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.9/site-packages (from requests->unstructured==0.11.8) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.9/site-packages (from requests->unstructured==0.11.8) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/vscode/.local/lib/python3.9/site-packages (from scikit-learn>=0.15->trectools) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from scikit-learn>=0.15->trectools) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/vscode/.local/lib/python3.9/site-packages (from beautifulsoup4->unstructured==0.11.8) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/vscode/.local/lib/python3.9/site-packages (from dataclasses-json->unstructured==0.11.8) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/vscode/.local/lib/python3.9/site-packages (from dataclasses-json->unstructured==0.11.8) (0.9.0)\n",
      "Requirement already satisfied: click in /home/vscode/.local/lib/python3.9/site-packages (from nltk->unstructured==0.11.8) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/vscode/.local/lib/python3.9/site-packages (from nltk->unstructured==0.11.8) (2023.12.25)\n",
      "Requirement already satisfied: jsonpath-python>=1.0.6 in /home/vscode/.local/lib/python3.9/site-packages (from unstructured-client->unstructured==0.11.8) (1.0.6)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /home/vscode/.local/lib/python3.9/site-packages (from unstructured-client->unstructured==0.11.8) (1.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/vscode/.local/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/vscode/.local/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=1.5->trectools) (3.17.0)\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=2ad95228b8f0a89c7d7400e752ac53c782df9e347280ded637172097e9a1eb67\n",
      "  Stored in directory: /home/vscode/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: absl-py, rouge-score\n",
      "Successfully installed absl-py-2.0.0 rouge-score-0.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trectools evaluate openai unstructured==0.11.8 pandas==2.1.4 azure-search-documents==11.4.0b11 python-dotenv==1.0.0 rouge-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "# this notebook will only use azure ai search, but feel free to extend\n",
    "# with other vector dbs\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.models import RawVectorQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings, \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "\n",
    "import ast\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from dotenv import dotenv_values\n",
    "import evaluate\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from openai.resources import Embeddings\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_values = dotenv_values()\n",
    "\n",
    "INDEX_NAME = \"evaluation-index\"\n",
    "\n",
    "class AzureAiSearchConfig(object):\n",
    "    _API_ENDPOINT_KEY = \"AIS_ENDPOINT\"\n",
    "    _API_VERSION_KEY = \"AIS_API_VERSION\"\n",
    "    _API_KEY_KEY = \"AIS_KEY\"\n",
    "\n",
    "    _REQUIRED_KEYS = [\n",
    "        _API_ENDPOINT_KEY,\n",
    "        _API_VERSION_KEY,\n",
    "        _API_KEY_KEY\n",
    "    ]\n",
    "\n",
    "    api_endpoint: str\n",
    "    api_version: str\n",
    "    api_key: str\n",
    "\n",
    "    def __init__(self, env_values: dict[str, any]):\n",
    "        _api_endpoint = env_values.get(AzureAiSearchConfig._API_ENDPOINT_KEY)\n",
    "        _api_version = env_values.get(AzureAiSearchConfig._API_VERSION_KEY)\n",
    "        _api_key = env_values.get(AzureAiSearchConfig._API_KEY_KEY)\n",
    "\n",
    "        if (not _api_endpoint or not _api_version or not _api_key):\n",
    "            raise ValueError(f\"The following environment variables are required: {', '.join(AzureAiSearchConfig._REQUIRED_KEYS)}\")\n",
    "\n",
    "        self.api_endpoint = _api_endpoint\n",
    "        self.api_version = _api_version\n",
    "        self.api_key = _api_key\n",
    "\n",
    "class AzureOpenAiConfig(object):\n",
    "    _API_ENDPOINT_KEY = \"AOAI_ENDPOINT\"\n",
    "    _API_VERSION_KEY = \"AOAI_API_VERSION\"\n",
    "    _API_KEY_KEY = \"AZURE_OPENAI_KEY\"\n",
    "    _DEPLOYMENT_MODEL_KEY = \"AOAI_EMBEDDING_DEPLOYED_MODEL\"\n",
    "\n",
    "    _REQUIRED_KEYS = [\n",
    "        _API_ENDPOINT_KEY,\n",
    "        _API_VERSION_KEY,\n",
    "        _API_KEY_KEY,\n",
    "        _DEPLOYMENT_MODEL_KEY\n",
    "    ]\n",
    "\n",
    "    api_endpoint: str\n",
    "    api_version: str\n",
    "    api_key: str\n",
    "    deployment_model: str\n",
    "\n",
    "    def __init__(self, env_values: dict[str, any]):\n",
    "        _api_endpoint = env_values.get(AzureOpenAiConfig._API_ENDPOINT_KEY)\n",
    "        _api_version = env_values.get(AzureOpenAiConfig._API_VERSION_KEY)\n",
    "        _api_key = env_values.get(AzureOpenAiConfig._API_KEY_KEY)\n",
    "        _deployment_model = env_values.get(AzureOpenAiConfig._DEPLOYMENT_MODEL_KEY)\n",
    "\n",
    "        if (not _api_endpoint or not _api_version or not _api_key):\n",
    "            raise ValueError(f\"The following environment variables are required: {', '.join(AzureOpenAiConfig._REQUIRED_KEYS)}\")\n",
    "\n",
    "        self.api_endpoint = _api_endpoint\n",
    "        self.api_version = _api_version\n",
    "        self.api_key = _api_key\n",
    "        self.deployment_model = _deployment_model\n",
    "\n",
    "\n",
    "azure_openai_config = AzureOpenAiConfig(env_values)\n",
    "azure_ai_search_config = AzureAiSearchConfig(env_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure search fields\n",
    "_ID_FIELD = \"id\"\n",
    "_CONTENT_FIELD = \"chunk_content\"\n",
    "_VECTOR_FIELD = \"chunk_content_vector\"\n",
    "_METADATA_FIELD = \"metadata\"\n",
    "_SOURCE_METADATA_FIELD = \"source\"\n",
    "\n",
    "_SELECT_FIELDS = [\n",
    "    _ID_FIELD,\n",
    "    _CONTENT_FIELD,\n",
    "    _VECTOR_FIELD,\n",
    "    _METADATA_FIELD,\n",
    "]\n",
    "\n",
    "# index config fields\n",
    "_DEFAULT_SEMANTIC_CONFIG_NAME = \"default\"\n",
    "_HNSW_ALGORITHM_CONFIG_NAME = \"hnsw_config\"\n",
    "_VECTOR_SEARCH_PROFILE_NAME = \"hnsw_profile\"\n",
    "\n",
    "# hnsw configs\n",
    "_M = 4\n",
    "_EF_CONSTRUCTION = 400\n",
    "_EF_SEARCH = 500\n",
    "_METRIC = \"cosine\"\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    id: str\n",
    "    content: str\n",
    "    score: float\n",
    "    source: str\n",
    "\n",
    "\n",
    "class _Embedding(object):\n",
    "    _embeddings: Embeddings\n",
    "    _embedding_model: str\n",
    "\n",
    "    def __init__(self, embeddings: Embeddings, embedding_model: str):\n",
    "        self._embeddings = embeddings\n",
    "        self._embedding_model = embedding_model\n",
    "\n",
    "    def embed(self, text: str):\n",
    "        return self._embeddings.create(input = [text], model=self._embedding_model).data[0].embedding\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureOpenAiConfig):\n",
    "        client = AzureOpenAI(\n",
    "            api_key = config.api_key,  \n",
    "            api_version = config.api_version,\n",
    "            azure_endpoint = config.api_endpoint\n",
    "        )\n",
    "        embeddings = client.embeddings\n",
    "        return cls(embeddings, config.deployment_model)\n",
    "\n",
    "\n",
    "class _SearchClient(object):\n",
    "    _BATCH_SIZE = 1000\n",
    "\n",
    "    search_client: SearchClient\n",
    "\n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self._search_client = search_client\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        embedding_vector: list[float],\n",
    "        k: int = 3,\n",
    "        vector_field: str = _VECTOR_FIELD,\n",
    "        select: list[str] = _SELECT_FIELDS,\n",
    "    ):\n",
    "        vector_query = RawVectorQuery(vector=embedding_vector, k=k, fields=vector_field)\n",
    "        return self._search_client.search(  \n",
    "            search_text=query,  \n",
    "            vector_queries= [vector_query],\n",
    "            select=_SELECT_FIELDS,\n",
    "        )\n",
    "\n",
    "    def upload(\n",
    "        self,\n",
    "        documents: list[dict]\n",
    "    ):\n",
    "        batch_size = _SearchClient._BATCH_SIZE\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            self._search_client.upload_documents(documents=batch)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureAiSearchConfig, index_name: str = INDEX_NAME):\n",
    "        search_client = SearchClient(config.api_endpoint, index_name, AzureKeyCredential(config.api_key))\n",
    "        return cls(search_client)\n",
    "\n",
    "class _SearchIndexClient(object):\n",
    "    _search_index_client: SearchIndexClient\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_index_client: SearchIndexClient\n",
    "    ):\n",
    "        self._search_index_client = search_index_client\n",
    "\n",
    "    def _get_index_client(self, index_name: str):\n",
    "        return self._search_index_client.get_index(index_name)\n",
    "\n",
    "    def _create_index_client(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        with_semantic_search: bool = False\n",
    "    ):\n",
    "        fields = [\n",
    "            SimpleField(name=_ID_FIELD, type=SearchFieldDataType.String, key=True),\n",
    "            SearchableField(name=_CONTENT_FIELD, type=SearchFieldDataType.String),\n",
    "            SearchField(name=_VECTOR_FIELD, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile=_VECTOR_SEARCH_PROFILE_NAME),\n",
    "            SearchableField(name=_METADATA_FIELD, type=SearchFieldDataType.String)\n",
    "        ]\n",
    "\n",
    "        vector_search = VectorSearch(\n",
    "            algorithms=[\n",
    "                HnswVectorSearchAlgorithmConfiguration(\n",
    "                    name=_HNSW_ALGORITHM_CONFIG_NAME,\n",
    "                    kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                    parameters=HnswParameters(\n",
    "                        m=_M,\n",
    "                        ef_construction=_EF_CONSTRUCTION,\n",
    "                        ef_search=_EF_SEARCH,\n",
    "                        metric=_METRIC\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            profiles=[\n",
    "                VectorSearchProfile(\n",
    "                    name=_VECTOR_SEARCH_PROFILE_NAME,\n",
    "                    algorithm=_HNSW_ALGORITHM_CONFIG_NAME\n",
    "                )\n",
    "            ]  \n",
    "        )\n",
    "\n",
    "        semantic_settings: SemanticSettings | None = None\n",
    "        if with_semantic_search:\n",
    "            semantic_settings = SemanticSettings(\n",
    "                configuration=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=_DEFAULT_SEMANTIC_CONFIG_NAME,\n",
    "                        prioritized_fields=PrioritizedFields(\n",
    "                            prioritized_content_field=[\n",
    "                                SemanticField(field_name=_CONTENT_FIELD),\n",
    "                                SemanticField(field_name=_METADATA_FIELD)\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=fields,\n",
    "            vector_search=vector_search,\n",
    "            semantic_settings=semantic_settings)\n",
    "        return self._search_index_client.create_index(index)\n",
    "    \n",
    "    def get_or_create_index_client(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        with_semantic_search: bool = False\n",
    "    ):\n",
    "        try:\n",
    "            return self._get_index_client(index_name)\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"Index {index_name} does not exist... Creating a new index.\")\n",
    "            return self._create_index_client(index_name, with_semantic_search)\n",
    "\n",
    "    def delete_index(self, index_name: str):\n",
    "        self._search_index_client.delete_index(index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureAiSearchConfig):\n",
    "        search_index_client = SearchIndexClient(config.api_endpoint, AzureKeyCredential(config.api_key))\n",
    "        return cls(search_index_client)\n",
    "\n",
    "\n",
    "class GroundTruthDatasetLoader(object):\n",
    "    _DATASET_PATH = \"../code_samples/data/thunderbolt/ground_truth/qa_dataset.csv\"\n",
    "\n",
    "    # df keys\n",
    "    _QUESTION_KEY = \"question\"\n",
    "    _ANSWER_KEY = \"answer\"\n",
    "    _SOURCE_KEY = \"source\"\n",
    "    _SEARCH_RESULT_KEY = \"search_result\"\n",
    "\n",
    "    _df: pd.DataFrame\n",
    "\n",
    "    def __init__(self):\n",
    "        self._df = pd.read_csv(GroundTruthDatasetLoader._DATASET_PATH)\n",
    "\n",
    "    def load_rows(self):\n",
    "        for i, row in self._df.iterrows():\n",
    "            yield i, row\n",
    "\n",
    "    def get_question_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._QUESTION_KEY]\n",
    "    \n",
    "    def get_answer_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._ANSWER_KEY]\n",
    "    \n",
    "    def get_source_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._SOURCE_KEY]\n",
    "    \n",
    "    def get_seach_answers_from_row(self, row: pd.Series) -> list[SearchResult]:\n",
    "        search_results = json.loads(row[GroundTruthDatasetLoader._SEARCH_RESULT_KEY])\n",
    "        return [SearchResult(**search_result) for search_result in search_results]\n",
    "\n",
    "    def set_search_answers_to_row(self, index: int, search_response: list[SearchResult]):\n",
    "        value = json.dumps([dataclasses.asdict(elem) for elem in search_response])\n",
    "        self.set_key(index, GroundTruthDatasetLoader._SEARCH_RESULT_KEY, value)\n",
    "\n",
    "    def set_key(self, index: int, key: str, value: any):\n",
    "        self._df.at[index, key] = value\n",
    "\n",
    "    def save(self, path: str):\n",
    "        self._df.to_json(path, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting index evaluation-index...\n",
      "Creating index and uploading data...\n",
      "Index evaluation-index does not exist... Creating a new index.\n",
      "Performing search...\n",
      "Evaluting chunks...\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n",
      "For recommender system, obtaining the ground truth is a complex problem in most cases as there is no way of identifying the ideal recommendation. For a retail website for example, click/not click, buy/not buy or other user interaction with recommendation can be used as ground truth proxies.\n"
     ]
    }
   ],
   "source": [
    "class ExperimentOrchestrator(object):\n",
    "    _IN_TOP_KEY = \"in_top\"\n",
    "    _ROUGE_L_KEY = \"rougeL\"\n",
    "\n",
    "    _DATASET_PATH = \"../code_samples/data/thunderbolt/embeddings.json\"\n",
    "    _dataset: list[dict]\n",
    "    _index_name: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_index_client: _SearchIndexClient,\n",
    "        search_client: _SearchClient,\n",
    "        embedding: _Embedding,\n",
    "        ground_truth_dataset_loader: GroundTruthDatasetLoader,\n",
    "        index_name: str = INDEX_NAME\n",
    "    ):\n",
    "        # delete the index\n",
    "        print(f\"Deleting index {index_name}...\")\n",
    "        search_index_client.delete_index(index_name)\n",
    "        time.sleep(2)\n",
    "\n",
    "        self._search_index_client = search_index_client\n",
    "        self._search_client = search_client\n",
    "        self._embedding = embedding\n",
    "        self._ground_truth_dataset_loader = ground_truth_dataset_loader\n",
    "        self._index_name = index_name\n",
    "        self._dataset = self._read_json_dataset(ExperimentOrchestrator._DATASET_PATH)\n",
    "\n",
    "    def _read_json_dataset(self, dataset_path: str):\n",
    "        with open(dataset_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def start(self):\n",
    "        # create the index if not exists and uploading the data\n",
    "        print(f\"Creating index and uploading data...\")\n",
    "        self._search_index_client.get_or_create_index_client(self._index_name)\n",
    "        self._search_client.upload(self._dataset)\n",
    "        time.sleep(2) # adding wait as documents may need to settle\n",
    "\n",
    "        print(\"Performing search...\")\n",
    "        for index, row in self._ground_truth_dataset_loader.load_rows():\n",
    "            question = self._ground_truth_dataset_loader.get_question_from_row(row)\n",
    "            answer = self._ground_truth_dataset_loader.get_answer_from_row(row)\n",
    "            embedding = self._embedding.embed(question)\n",
    "            search_result_paged = self._search_client.search(question, embedding)\n",
    "\n",
    "            search_results_final: list[SearchResult] = []\n",
    "            for item in search_result_paged:\n",
    "                del item[_VECTOR_FIELD]\n",
    "                source = json.loads(item[_METADATA_FIELD])[_SOURCE_METADATA_FIELD]\n",
    "                search_result = SearchResult(\n",
    "                    id=item[_ID_FIELD],\n",
    "                    content=item[_CONTENT_FIELD],\n",
    "                    score=item[\"@search.score\"],\n",
    "                    source=source\n",
    "                )\n",
    "                search_results_final.append(search_result)\n",
    "            self._ground_truth_dataset_loader.set_search_answers_to_row(index, search_results_final)\n",
    "\n",
    "        print(\"Evaluting chunks...\")\n",
    "        for index, row in self._ground_truth_dataset_loader.load_rows():\n",
    "            # calculate in top init\n",
    "            expected_source = self._ground_truth_dataset_loader.get_source_from_row(row)\n",
    "            search_results = self._ground_truth_dataset_loader.get_seach_answers_from_row(row)\n",
    "            search_sources = [search_result.source for search_result in search_results]\n",
    "\n",
    "            contains_source = 1 if expected_source in search_sources else 0\n",
    "            self._ground_truth_dataset_loader.set_key(index, ExperimentOrchestrator._IN_TOP_KEY, contains_source)\n",
    "\n",
    "            # rouge L metric\n",
    "            # create code for rouge L score\n",
    "            search_answers = [search_result.content for search_result in search_results]\n",
    "            # print(answer)\n",
    "\n",
    "            predictions = [search_answers[0]]\n",
    "            references = [[answer]]\n",
    "            results = rouge.compute(predictions=predictions, references=references)\n",
    "            rouge_l_score = results['rougeL']\n",
    "            \n",
    "            self._ground_truth_dataset_loader.set_key(index, ExperimentOrchestrator._ROUGE_L_KEY, rouge_l_score)\n",
    "\n",
    "        self._ground_truth_dataset_loader.save(\"thunderbolt.json\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls,\n",
    "        azure_ai_search_config: AzureAiSearchConfig,\n",
    "        azure_openai_config: AzureOpenAiConfig,\n",
    "        index_name: str\n",
    "    ):\n",
    "        search_client_index = _SearchIndexClient.from_config(azure_ai_search_config)\n",
    "        search_client = _SearchClient.from_config(azure_ai_search_config, index_name)\n",
    "        embedding = _Embedding.from_config(azure_openai_config)\n",
    "        \n",
    "        return cls(search_client_index, search_client, embedding, GroundTruthDatasetLoader(), index_name)\n",
    "\n",
    "orchestrator = ExperimentOrchestrator.from_config(azure_ai_search_config, azure_openai_config, INDEX_NAME)\n",
    "orchestrator.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
