{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary packages\n",
    "# this notebook will only use azure ai search, but feel free to extend\n",
    "# with other vector dbs\n",
    "\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.search.documents.models import RawVectorQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSettings, \n",
    "    VectorSearch,  \n",
    "    HnswVectorSearchAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile\n",
    ")\n",
    "\n",
    "import ast\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from dotenv import dotenv_values\n",
    "import evaluate\n",
    "import json\n",
    "from openai import AzureOpenAI\n",
    "from openai.resources import Embeddings\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_values = dotenv_values()\n",
    "\n",
    "INDEX_NAME = \"evaluation-index\"\n",
    "\n",
    "class AzureAiSearchConfig(object):\n",
    "    _API_ENDPOINT_KEY = \"AIS_ENDPOINT\"\n",
    "    _API_VERSION_KEY = \"AIS_API_VERSION\"\n",
    "    _API_KEY_KEY = \"AIS_KEY\"\n",
    "\n",
    "    _REQUIRED_KEYS = [\n",
    "        _API_ENDPOINT_KEY,\n",
    "        _API_VERSION_KEY,\n",
    "        _API_KEY_KEY\n",
    "    ]\n",
    "\n",
    "    api_endpoint: str\n",
    "    api_version: str\n",
    "    api_key: str\n",
    "\n",
    "    def __init__(self, env_values: dict[str, any]):\n",
    "        _api_endpoint = env_values.get(AzureAiSearchConfig._API_ENDPOINT_KEY)\n",
    "        _api_version = env_values.get(AzureAiSearchConfig._API_VERSION_KEY)\n",
    "        _api_key = env_values.get(AzureAiSearchConfig._API_KEY_KEY)\n",
    "\n",
    "        if (not _api_endpoint or not _api_version or not _api_key):\n",
    "            raise ValueError(f\"The following environment variables are required: {', '.join(AzureAiSearchConfig._REQUIRED_KEYS)}\")\n",
    "\n",
    "        self.api_endpoint = _api_endpoint\n",
    "        self.api_version = _api_version\n",
    "        self.api_key = _api_key\n",
    "\n",
    "class AzureOpenAiConfig(object):\n",
    "    _API_ENDPOINT_KEY = \"AOAI_ENDPOINT\"\n",
    "    _API_VERSION_KEY = \"AOAI_API_VERSION\"\n",
    "    _API_KEY_KEY = \"AZURE_OPENAI_KEY\"\n",
    "    _DEPLOYMENT_MODEL_KEY = \"AOAI_EMBEDDING_DEPLOYED_MODEL\"\n",
    "\n",
    "    _REQUIRED_KEYS = [\n",
    "        _API_ENDPOINT_KEY,\n",
    "        _API_VERSION_KEY,\n",
    "        _API_KEY_KEY,\n",
    "        _DEPLOYMENT_MODEL_KEY\n",
    "    ]\n",
    "\n",
    "    api_endpoint: str\n",
    "    api_version: str\n",
    "    api_key: str\n",
    "    deployment_model: str\n",
    "\n",
    "    def __init__(self, env_values: dict[str, any]):\n",
    "        _api_endpoint = env_values.get(AzureOpenAiConfig._API_ENDPOINT_KEY)\n",
    "        _api_version = env_values.get(AzureOpenAiConfig._API_VERSION_KEY)\n",
    "        _api_key = env_values.get(AzureOpenAiConfig._API_KEY_KEY)\n",
    "        _deployment_model = env_values.get(AzureOpenAiConfig._DEPLOYMENT_MODEL_KEY)\n",
    "\n",
    "        if (not _api_endpoint or not _api_version or not _api_key):\n",
    "            raise ValueError(f\"The following environment variables are required: {', '.join(AzureOpenAiConfig._REQUIRED_KEYS)}\")\n",
    "\n",
    "        self.api_endpoint = _api_endpoint\n",
    "        self.api_version = _api_version\n",
    "        self.api_key = _api_key\n",
    "        self.deployment_model = _deployment_model\n",
    "\n",
    "\n",
    "azure_openai_config = AzureOpenAiConfig(env_values)\n",
    "azure_ai_search_config = AzureAiSearchConfig(env_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azure search fields\n",
    "_ID_FIELD = \"id\"\n",
    "_CONTENT_FIELD = \"chunk_content\"\n",
    "_VECTOR_FIELD = \"chunk_content_vector\"\n",
    "_METADATA_FIELD = \"metadata\"\n",
    "_SOURCE_METADATA_FIELD = \"source\"\n",
    "\n",
    "_SELECT_FIELDS = [\n",
    "    _ID_FIELD,\n",
    "    _CONTENT_FIELD,\n",
    "    _VECTOR_FIELD,\n",
    "    _METADATA_FIELD,\n",
    "]\n",
    "\n",
    "# index config fields\n",
    "_DEFAULT_SEMANTIC_CONFIG_NAME = \"default\"\n",
    "_HNSW_ALGORITHM_CONFIG_NAME = \"hnsw_config\"\n",
    "_VECTOR_SEARCH_PROFILE_NAME = \"hnsw_profile\"\n",
    "\n",
    "# hnsw configs\n",
    "_M = 4\n",
    "_EF_CONSTRUCTION = 400\n",
    "_EF_SEARCH = 500\n",
    "_METRIC = \"cosine\"\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    id: str\n",
    "    content: str\n",
    "    score: float\n",
    "    source: str\n",
    "\n",
    "\n",
    "class _Embedding(object):\n",
    "    _embeddings: Embeddings\n",
    "    _embedding_model: str\n",
    "\n",
    "    def __init__(self, embeddings: Embeddings, embedding_model: str):\n",
    "        self._embeddings = embeddings\n",
    "        self._embedding_model = embedding_model\n",
    "\n",
    "    def embed(self, text: str):\n",
    "        return self._embeddings.create(input = [text], model=self._embedding_model).data[0].embedding\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureOpenAiConfig):\n",
    "        client = AzureOpenAI(\n",
    "            api_key = config.api_key,  \n",
    "            api_version = config.api_version,\n",
    "            azure_endpoint = config.api_endpoint\n",
    "        )\n",
    "        embeddings = client.embeddings\n",
    "        return cls(embeddings, config.deployment_model)\n",
    "\n",
    "\n",
    "class _SearchClient(object):\n",
    "    _BATCH_SIZE = 1000\n",
    "\n",
    "    search_client: SearchClient\n",
    "\n",
    "    def __init__(self, search_client: SearchClient):\n",
    "        self._search_client = search_client\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        embedding_vector: list[float],\n",
    "        k: int = 3,\n",
    "        vector_field: str = _VECTOR_FIELD,\n",
    "        select: list[str] = _SELECT_FIELDS,\n",
    "    ):\n",
    "        vector_query = RawVectorQuery(vector=embedding_vector, k=k, fields=vector_field)\n",
    "        return self._search_client.search(  \n",
    "            search_text=query,  \n",
    "            vector_queries= [vector_query],\n",
    "            select=_SELECT_FIELDS,\n",
    "        )\n",
    "\n",
    "    def upload(\n",
    "        self,\n",
    "        documents: list[dict]\n",
    "    ):\n",
    "        batch_size = _SearchClient._BATCH_SIZE\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            self._search_client.upload_documents(documents=batch)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureAiSearchConfig, index_name: str = INDEX_NAME):\n",
    "        search_client = SearchClient(config.api_endpoint, index_name, AzureKeyCredential(config.api_key))\n",
    "        return cls(search_client)\n",
    "\n",
    "class _SearchIndexClient(object):\n",
    "    _search_index_client: SearchIndexClient\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_index_client: SearchIndexClient\n",
    "    ):\n",
    "        self._search_index_client = search_index_client\n",
    "\n",
    "    def _get_index_client(self, index_name: str):\n",
    "        return self._search_index_client.get_index(index_name)\n",
    "\n",
    "    def _create_index_client(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        with_semantic_search: bool = False\n",
    "    ):\n",
    "        fields = [\n",
    "            SimpleField(name=_ID_FIELD, type=SearchFieldDataType.String, key=True),\n",
    "            SearchableField(name=_CONTENT_FIELD, type=SearchFieldDataType.String),\n",
    "            SearchField(name=_VECTOR_FIELD, type=SearchFieldDataType.Collection(SearchFieldDataType.Single), searchable=True, vector_search_dimensions=1536, vector_search_profile=_VECTOR_SEARCH_PROFILE_NAME),\n",
    "            SearchableField(name=_METADATA_FIELD, type=SearchFieldDataType.String)\n",
    "        ]\n",
    "\n",
    "        vector_search = VectorSearch(\n",
    "            algorithms=[\n",
    "                HnswVectorSearchAlgorithmConfiguration(\n",
    "                    name=_HNSW_ALGORITHM_CONFIG_NAME,\n",
    "                    kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                    parameters=HnswParameters(\n",
    "                        m=_M,\n",
    "                        ef_construction=_EF_CONSTRUCTION,\n",
    "                        ef_search=_EF_SEARCH,\n",
    "                        metric=_METRIC\n",
    "                    )\n",
    "                )\n",
    "            ],\n",
    "            profiles=[\n",
    "                VectorSearchProfile(\n",
    "                    name=_VECTOR_SEARCH_PROFILE_NAME,\n",
    "                    algorithm=_HNSW_ALGORITHM_CONFIG_NAME\n",
    "                )\n",
    "            ]  \n",
    "        )\n",
    "\n",
    "        semantic_settings: SemanticSettings | None = None\n",
    "        if with_semantic_search:\n",
    "            semantic_settings = SemanticSettings(\n",
    "                configuration=[\n",
    "                    SemanticConfiguration(\n",
    "                        name=_DEFAULT_SEMANTIC_CONFIG_NAME,\n",
    "                        prioritized_fields=PrioritizedFields(\n",
    "                            prioritized_content_field=[\n",
    "                                SemanticField(field_name=_CONTENT_FIELD),\n",
    "                                SemanticField(field_name=_METADATA_FIELD)\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        index = SearchIndex(\n",
    "            name=index_name,\n",
    "            fields=fields,\n",
    "            vector_search=vector_search,\n",
    "            semantic_settings=semantic_settings)\n",
    "        return self._search_index_client.create_index(index)\n",
    "    \n",
    "    def get_or_create_index_client(\n",
    "        self,\n",
    "        index_name: str,\n",
    "        with_semantic_search: bool = False\n",
    "    ):\n",
    "        try:\n",
    "            return self._get_index_client(index_name)\n",
    "        except ResourceNotFoundError:\n",
    "            print(f\"Index {index_name} does not exist... Creating a new index.\")\n",
    "            return self._create_index_client(index_name, with_semantic_search)\n",
    "\n",
    "    def delete_index(self, index_name: str):\n",
    "        self._search_index_client.delete_index(index_name)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: AzureAiSearchConfig):\n",
    "        search_index_client = SearchIndexClient(config.api_endpoint, AzureKeyCredential(config.api_key))\n",
    "        return cls(search_index_client)\n",
    "\n",
    "\n",
    "class GroundTruthDatasetLoader(object):\n",
    "    _DATASET_PATH = \"../code_samples/data/thunderbolt/ground_truth/qa_dataset.csv\"\n",
    "\n",
    "    # df keys\n",
    "    _QUESTION_KEY = \"question\"\n",
    "    _ANSWER_KEY = \"answer\"\n",
    "    _SOURCE_KEY = \"source\"\n",
    "    _SEARCH_RESULT_KEY = \"search_result\"\n",
    "\n",
    "    _df: pd.DataFrame\n",
    "\n",
    "    def __init__(self):\n",
    "        self._df = pd.read_csv(GroundTruthDatasetLoader._DATASET_PATH)\n",
    "\n",
    "    def load_rows(self):\n",
    "        for i, row in self._df.iterrows():\n",
    "            yield i, row\n",
    "\n",
    "    def get_question_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._QUESTION_KEY]\n",
    "    \n",
    "    def get_answer_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._ANSWER_KEY]\n",
    "    \n",
    "    def get_source_from_row(self, row: pd.Series):\n",
    "        return row[GroundTruthDatasetLoader._SOURCE_KEY]\n",
    "    \n",
    "    def get_seach_answers_from_row(self, row: pd.Series) -> list[SearchResult]:\n",
    "        search_results = json.loads(row[GroundTruthDatasetLoader._SEARCH_RESULT_KEY])\n",
    "        return [SearchResult(**search_result) for search_result in search_results]\n",
    "\n",
    "    def set_search_answers_to_row(self, index: int, search_response: list[SearchResult]):\n",
    "        value = json.dumps([dataclasses.asdict(elem) for elem in search_response])\n",
    "        self.set_key(index, GroundTruthDatasetLoader._SEARCH_RESULT_KEY, value)\n",
    "\n",
    "    def set_key(self, index: int, key: str, value: any):\n",
    "        self._df.at[index, key] = value\n",
    "\n",
    "    def save(self, path: str):\n",
    "        self._df.to_json(path, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting index evaluation-index...\n",
      "Creating index and uploading data...\n",
      "Index evaluation-index does not exist... Creating a new index.\n",
      "Performing search...\n",
      "Evaluting chunks...\n"
     ]
    }
   ],
   "source": [
    "class ExperimentOrchestrator(object):\n",
    "    _IN_TOP_KEY = \"in_top\"\n",
    "\n",
    "    _DATASET_PATH = \"../code_samples/data/thunderbolt/embeddings.json\"\n",
    "    _dataset: list[dict]\n",
    "    _index_name: str\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        search_index_client: _SearchIndexClient,\n",
    "        search_client: _SearchClient,\n",
    "        embedding: _Embedding,\n",
    "        ground_truth_dataset_loader: GroundTruthDatasetLoader,\n",
    "        index_name: str = INDEX_NAME\n",
    "    ):\n",
    "        # delete the index\n",
    "        print(f\"Deleting index {index_name}...\")\n",
    "        search_index_client.delete_index(index_name)\n",
    "        time.sleep(2)\n",
    "\n",
    "        self._search_index_client = search_index_client\n",
    "        self._search_client = search_client\n",
    "        self._embedding = embedding\n",
    "        self._ground_truth_dataset_loader = ground_truth_dataset_loader\n",
    "        self._index_name = index_name\n",
    "        self._dataset = self._read_json_dataset(ExperimentOrchestrator._DATASET_PATH)\n",
    "\n",
    "    def _read_json_dataset(self, dataset_path: str):\n",
    "        with open(dataset_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def start(self):\n",
    "        # create the index if not exists and uploading the data\n",
    "        print(f\"Creating index and uploading data...\")\n",
    "        self._search_index_client.get_or_create_index_client(self._index_name)\n",
    "        self._search_client.upload(self._dataset)\n",
    "        time.sleep(2) # adding wait as documents may need to settle\n",
    "\n",
    "        print(\"Performing search...\")\n",
    "        for index, row in self._ground_truth_dataset_loader.load_rows():\n",
    "            question = self._ground_truth_dataset_loader.get_question_from_row(row)\n",
    "            embedding = self._embedding.embed(question)\n",
    "            search_result_paged = self._search_client.search(question, embedding)\n",
    "\n",
    "            search_results_final: list[SearchResult] = []\n",
    "            for item in search_result_paged:\n",
    "                del item[_VECTOR_FIELD]\n",
    "                source = json.loads(item[_METADATA_FIELD])[_SOURCE_METADATA_FIELD]\n",
    "                search_result = SearchResult(\n",
    "                    id=item[_ID_FIELD],\n",
    "                    content=item[_CONTENT_FIELD],\n",
    "                    score=item[\"@search.score\"],\n",
    "                    source=source\n",
    "                )\n",
    "                search_results_final.append(search_result)\n",
    "            self._ground_truth_dataset_loader.set_search_answers_to_row(index, search_results_final)\n",
    "\n",
    "        print(\"Evaluting chunks...\")\n",
    "        for index, row in self._ground_truth_dataset_loader.load_rows():\n",
    "            # calculate in top init\n",
    "            expected_source = self._ground_truth_dataset_loader.get_source_from_row(row)\n",
    "            search_results = self._ground_truth_dataset_loader.get_seach_answers_from_row(row)\n",
    "            search_sources = [search_result.source for search_result in search_results]\n",
    "\n",
    "            contains_source = 1 if expected_source in search_sources else 0\n",
    "            self._ground_truth_dataset_loader.set_key(index, ExperimentOrchestrator._IN_TOP_KEY, contains_source)\n",
    "\n",
    "            # rouge L metric\n",
    "            # create code for rouge L score\n",
    "        self._ground_truth_dataset_loader.save(\"thunderbolt.json\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(\n",
    "        cls,\n",
    "        azure_ai_search_config: AzureAiSearchConfig,\n",
    "        azure_openai_config: AzureOpenAiConfig,\n",
    "        index_name: str\n",
    "    ):\n",
    "        search_client_index = _SearchIndexClient.from_config(azure_ai_search_config)\n",
    "        search_client = _SearchClient.from_config(azure_ai_search_config, index_name)\n",
    "        embedding = _Embedding.from_config(azure_openai_config)\n",
    "        \n",
    "        return cls(search_client_index, search_client, embedding, GroundTruthDatasetLoader(), index_name)\n",
    "\n",
    "orchestrator = ExperimentOrchestrator.from_config(azure_ai_search_config, azure_openai_config, INDEX_NAME)\n",
    "orchestrator.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
