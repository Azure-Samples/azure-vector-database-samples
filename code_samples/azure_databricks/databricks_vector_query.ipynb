{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e81a46e-d8e3-4b9c-b6fd-0e23bb661834",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Vector Search Python SDK example usage\n",
    "\n",
    "This notebook demonstrates usage of the Vector Search Python SDK, which provides a `VectorSearchClient` as a primary API for working with Vector Search.\n",
    "\n",
    "Alternatively, you may call the REST API directly.\n",
    "\n",
    "**Pre-req**: This notebook assumes you have already created a Model Serving endpoint for the embedding model.  See `embedding_model_endpoint` below, and the companion notebook for creating endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0216d136-de35-4f8b-8ae0-7e0343ec8dd5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Similarity search\n",
    "\n",
    "Query the Vector Index to find similar documents!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "059b3d5d-37c9-47ff-801d-2f06c8b80d37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting databricks-vectorsearch\n  Using cached databricks_vectorsearch-0.22-py3-none-any.whl (8.5 kB)\nCollecting mlflow-skinny<3,>=2.4.0\n  Using cached mlflow_skinny-2.10.2-py3-none-any.whl (4.8 MB)\nCollecting protobuf<5,>=3.12.0\n  Using cached protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\nCollecting requests>=2\n  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\nCollecting cloudpickle<4\n  Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\n  Using cached importlib_metadata-7.0.1-py3-none-any.whl (23 kB)\nCollecting entrypoints<1\n  Using cached entrypoints-0.4-py3-none-any.whl (5.3 kB)\nCollecting packaging<24\n  Using cached packaging-23.2-py3-none-any.whl (53 kB)\nCollecting pytz<2024\n  Using cached pytz-2023.4-py2.py3-none-any.whl (506 kB)\nCollecting pyyaml<7,>=5.1\n  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\nCollecting gitpython<4,>=2.1.0\n  Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\nCollecting click<9,>=7.0\n  Using cached click-8.1.7-py3-none-any.whl (97 kB)\nCollecting sqlparse<1,>=0.4.0\n  Using cached sqlparse-0.4.4-py3-none-any.whl (41 kB)\nCollecting urllib3<3,>=1.21.1\n  Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\nCollecting idna<4,>=2.5\n  Using cached idna-3.6-py3-none-any.whl (61 kB)\nCollecting charset-normalizer<4,>=2\n  Using cached charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\nCollecting certifi>=2017.4.17\n  Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\nCollecting gitdb<5,>=4.0.1\n  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\nCollecting zipp>=0.5\n  Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\nCollecting smmap<6,>=3.0.1\n  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\nInstalling collected packages: pytz, zipp, urllib3, sqlparse, smmap, pyyaml, protobuf, packaging, idna, entrypoints, cloudpickle, click, charset-normalizer, certifi, requests, importlib-metadata, gitdb, gitpython, mlflow-skinny, databricks-vectorsearch\n  Attempting uninstall: pytz\n    Found existing installation: pytz 2023.4\n    Uninstalling pytz-2023.4:\n      Successfully uninstalled pytz-2023.4\n  Attempting uninstall: zipp\n    Found existing installation: zipp 3.17.0\n    Uninstalling zipp-3.17.0:\n      Successfully uninstalled zipp-3.17.0\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.2.1\n    Uninstalling urllib3-2.2.1:\n      Successfully uninstalled urllib3-2.2.1\n  Attempting uninstall: sqlparse\n    Found existing installation: sqlparse 0.4.4\n    Uninstalling sqlparse-0.4.4:\n      Successfully uninstalled sqlparse-0.4.4\n  Attempting uninstall: smmap\n    Found existing installation: smmap 5.0.1\n    Uninstalling smmap-5.0.1:\n      Successfully uninstalled smmap-5.0.1\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 6.0.1\n    Uninstalling PyYAML-6.0.1:\n      Successfully uninstalled PyYAML-6.0.1\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.3\n    Uninstalling protobuf-4.25.3:\n      Successfully uninstalled protobuf-4.25.3\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.2\n    Uninstalling packaging-23.2:\n      Successfully uninstalled packaging-23.2\n  Attempting uninstall: idna\n    Found existing installation: idna 3.6\n    Uninstalling idna-3.6:\n      Successfully uninstalled idna-3.6\n  Attempting uninstall: entrypoints\n    Found existing installation: entrypoints 0.4\n    Uninstalling entrypoints-0.4:\n      Successfully uninstalled entrypoints-0.4\n  Attempting uninstall: cloudpickle\n    Found existing installation: cloudpickle 3.0.0\n    Uninstalling cloudpickle-3.0.0:\n      Successfully uninstalled cloudpickle-3.0.0\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Uninstalling click-8.1.7:\n      Successfully uninstalled click-8.1.7\n  Attempting uninstall: charset-normalizer\n    Found existing installation: charset-normalizer 3.3.2\n    Uninstalling charset-normalizer-3.3.2:\n      Successfully uninstalled charset-normalizer-3.3.2\n  Attempting uninstall: certifi\n    Found existing installation: certifi 2024.2.2\n    Uninstalling certifi-2024.2.2:\n      Successfully uninstalled certifi-2024.2.2\n  Attempting uninstall: requests\n    Found existing installation: requests 2.31.0\n    Uninstalling requests-2.31.0:\n      Successfully uninstalled requests-2.31.0\n  Attempting uninstall: importlib-metadata\n    Found existing installation: importlib-metadata 7.0.1\n    Uninstalling importlib-metadata-7.0.1:\n      Successfully uninstalled importlib-metadata-7.0.1\n  Attempting uninstall: gitdb\n    Found existing installation: gitdb 4.0.11\n    Uninstalling gitdb-4.0.11:\n      Successfully uninstalled gitdb-4.0.11\n  Attempting uninstall: gitpython\n    Found existing installation: GitPython 3.1.42\n    Uninstalling GitPython-3.1.42:\n      Successfully uninstalled GitPython-3.1.42\n  Attempting uninstall: mlflow-skinny\n    Found existing installation: mlflow-skinny 2.10.2\n    Uninstalling mlflow-skinny-2.10.2:\n      Successfully uninstalled mlflow-skinny-2.10.2\n  Attempting uninstall: databricks-vectorsearch\n    Found existing installation: databricks-vectorsearch 0.22\n    Uninstalling databricks-vectorsearch-0.22:\n      Successfully uninstalled databricks-vectorsearch-0.22\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.14.3 requires pyspark<4,>=3.1.2, which is not installed.\ntensorflow-cpu 2.11.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\ntensorboard 2.11.0 requires protobuf<4,>=3.9.2, but you have protobuf 4.25.3 which is incompatible.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.1.1 which is incompatible.\ndatabricks-sdk 0.1.6 requires requests<2.29.0,>=2.28.1, but you have requests 2.31.0 which is incompatible.\ndatabricks-cli 0.17.7 requires urllib3<2.0.0,>=1.26.7, but you have urllib3 2.2.1 which is incompatible.\nbotocore 1.27.28 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\nSuccessfully installed certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-3.0.0 databricks-vectorsearch-0.22 entrypoints-0.4 gitdb-4.0.11 gitpython-3.1.42 idna-3.6 importlib-metadata-7.0.1 mlflow-skinny-2.10.2 packaging-23.2 protobuf-4.25.3 pytz-2023.4 pyyaml-6.0.1 requests-2.31.0 smmap-5.0.1 sqlparse-0.4.4 urllib3-2.2.1 zipp-3.17.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --force-reinstall databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca630fcd-755e-44d2-bfba-a49cf1ac7855",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NOTICE] Using a notebook authentication token. Recommended for development only. For improved performance, please use Service Principal based authentication. To disable this message, pass disable_notice=True to VectorSearchClient().\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "# Automatically generates a PAT Token for authentication\n",
    "vsc = VectorSearchClient()\n",
    "\n",
    "# Uses the service principal token for authentication\n",
    "# client = VectorSearch(service_principal_client_id=<CLIENT_ID>,service_principal_client_secret=<CLIENT_SECRET>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b627e87c-1ce6-41c4-957e-1d662de7f759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_catalog = \"vector_database\"\n",
    "source_schema = \"vector_search\"\n",
    "source_table = \"product\"\n",
    "source_table_fullname = f\"{source_catalog}.{source_schema}.{source_table}\"\n",
    "vs_index = \"product_vsindex\"\n",
    "vector_search_endpoint_name = \"vector-search-demo-endpoint\"\n",
    "vs_index_fullname = f\"{source_catalog}.{source_schema}.{vs_index}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430cefcf-a58a-4ad7-9ded-5db60d9bec2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'name': 'vector_database.vector_search.product_vsindex1',\n",
       " 'endpoint_name': 'vector-search-demo-endpoint',\n",
       " 'primary_key': 'id',\n",
       " 'index_type': 'DELTA_SYNC',\n",
       " 'delta_sync_index_spec': {'source_table': 'vector_database.vector_search.product',\n",
       "  'embedding_source_columns': [{'name': 'content',\n",
       "    'embedding_model_endpoint_name': 'databricks-bge-large-en'}],\n",
       "  'pipeline_type': 'TRIGGERED',\n",
       "  'pipeline_id': 'ae9926f6-b87e-46f3-98b5-bf60eebb3edf'},\n",
       " 'status': {'detailed_state': 'ONLINE_NO_PENDING_UPDATE',\n",
       "  'message': 'Index creation succeeded. Check latest status: https://adb-2878440741389728.8.azuredatabricks.net/explore/data/vector_database/vector_search/product_vsindex1',\n",
       "  'indexed_row_count': 108,\n",
       "  'triggered_update_status': {'last_processed_commit_version': 0,\n",
       "   'last_processed_commit_timestamp': '2024-02-20T05:46:13Z'},\n",
       "  'ready': True,\n",
       "  'index_url': 'adb-2878440741389728.8.azuredatabricks.net/api/2.0/vector-search/endpoints/vector-search-demo-endpoint/indexes/vector_database.vector_search.product_vsindex1'},\n",
       " 'creator': 'hema.verma@mngenvmcap040685.onmicrosoft.com'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = vsc.get_index(endpoint_name=vector_search_endpoint_name, index_name=vs_index_fullname)\n",
    "index.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "50527b6b-53cd-4cfa-becd-e5f35c606a42",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Performing Similarity Search and converting the results to a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "839e42b2-5ced-4720-a1ed-90cfa51120f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>comment</th><th>id</th><th>title</th><th>distance</th></tr></thead><tbody><tr><td>Databases</td><td>Azure Database for MySQL is a fully managed, scalable, and secure relational database service that enables you to build and manage MySQL applications in Azure. It provides features like automatic backups, monitoring, and high availability. Database for MySQL supports various data types, such as JSON, spatial, and full-text. You can use Azure Database for MySQL to migrate your existing applications, build new applications, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure App Service and Azure Data Factory.</td><td>66</td><td>Azure Database for MySQL</td><td>0.5787114</td></tr><tr><td>Databases</td><td>Azure Database for MariaDB is a fully managed, scalable, and secure relational database service that enables you to build and manage MariaDB applications in Azure. It provides features like automatic backups, monitoring, and high availability. Database for MariaDB supports various data types, such as JSON, spatial, and full-text. You can use Azure Database for MariaDB to migrate your existing applications, build new applications, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure App Service and Azure Data Factory.</td><td>68</td><td>Azure Database for MariaDB</td><td>0.56967765</td></tr><tr><td>Databases</td><td>Azure SQL Database is a fully managed relational database service based on the latest stable version of Microsoft SQL Server. It offers built-in intelligence that learns your application patterns and adapts to maximize performance, reliability, and data protection. SQL Database supports elastic scaling, allowing you to dynamically adjust resources to match your workload. It provides advanced security features, such as encryption, auditing, and threat detection. You can migrate your existing SQL Server databases to Azure SQL Database with minimal downtime.</td><td>5</td><td>Azure SQL Database</td><td>0.5664096</td></tr><tr><td>Databases</td><td>Azure Cosmos DB is a fully managed, globally distributed, multi-model database service designed for building highly responsive and scalable applications. It offers turnkey global distribution, automatic and instant scalability, and guarantees low latency, high availability, and consistency. Cosmos DB supports popular NoSQL APIs, including MongoDB, Cassandra, Gremlin, and Azure Table Storage. You can build globally distributed applications with ease, without having to deal with complex configuration and capacity planning. Data stored in Cosmos DB is automatically indexed, enabling you to query your data with SQL, JavaScript, or other supported query languages.</td><td>6</td><td>Azure Cosmos DB</td><td>0.5639423</td></tr><tr><td>Databases</td><td>Azure Database for PostgreSQL is a fully managed, scalable, and secure relational database service that enables you to build and manage PostgreSQL applications in Azure. It provides features like automatic backups, monitoring, and high availability. Database for PostgreSQL supports various data types, such as JSON, spatial, and full-text. You can use Azure Database for PostgreSQL to migrate your existing applications, build new applications, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure App Service and Azure Data Factory.</td><td>67</td><td>Azure Database for PostgreSQL</td><td>0.5619546</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Databases",
         "Azure Database for MySQL is a fully managed, scalable, and secure relational database service that enables you to build and manage MySQL applications in Azure. It provides features like automatic backups, monitoring, and high availability. Database for MySQL supports various data types, such as JSON, spatial, and full-text. You can use Azure Database for MySQL to migrate your existing applications, build new applications, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure App Service and Azure Data Factory.",
         "66",
         "Azure Database for MySQL",
         0.5787114
        ],
        [
         "Databases",
         "Azure Database for MariaDB is a fully managed, scalable, and secure relational database service that enables you to build and manage MariaDB applications in Azure. It provides features like automatic backups, monitoring, and high availability. Database for MariaDB supports various data types, such as JSON, spatial, and full-text. You can use Azure Database for MariaDB to migrate your existing applications, build new applications, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure App Service and Azure Data Factory.",
         "68",
         "Azure Database for MariaDB",
         0.56967765
        ],
        [
         "Databases",
         "Azure SQL Database is a fully managed relational database service based on the latest stable version of Microsoft SQL Server. It offers built-in intelligence that learns your application patterns and adapts to maximize performance, reliability, and data protection. SQL Database supports elastic scaling, allowing you to dynamically adjust resources to match your workload. It provides advanced security features, such as encryption, auditing, and threat detection. You can migrate your existing SQL Server databases to Azure SQL Database with minimal downtime.",
         "5",
         "Azure SQL Database",
         0.5664096
        ],
        [
         "Databases",
         "Azure Cosmos DB is a fully managed, globally distributed, multi-model database service designed for building highly responsive and scalable applications. It offers turnkey global distribution, automatic and instant scalability, and guarantees low latency, high availability, and consistency. Cosmos DB supports popular NoSQL APIs, including MongoDB, Cassandra, Gremlin, and Azure Table Storage. You can build globally distributed applications with ease, without having to deal with complex configuration and capacity planning. Data stored in Cosmos DB is automatically indexed, enabling you to query your data with SQL, JavaScript, or other supported query languages.",
         "6",
         "Azure Cosmos DB",
         0.5639423
        ],
        [
         "Databases",
         "Azure Database for PostgreSQL is a fully managed, scalable, and secure relational database service that enables you to build and manage PostgreSQL applications in Azure. It provides features like automatic backups, monitoring, and high availability. Database for PostgreSQL supports various data types, such as JSON, spatial, and full-text. You can use Azure Database for PostgreSQL to migrate your existing applications, build new applications, and ensure the performance and security of your data. It also integrates with other Azure services, such as Azure App Service and Azure Data Factory.",
         "67",
         "Azure Database for PostgreSQL",
         0.5619546
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "distance",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DoubleType\n",
    "all_columns = spark.table(source_table_fullname).columns\n",
    "\n",
    "results = index.similarity_search(\n",
    "  query_text=\"Databases\",\n",
    "  columns=all_columns)\n",
    "\n",
    "ls_results= results.get('result').get('data_array')\n",
    "df = spark.createDataFrame(data = ls_results, schema = \"category STRING , comment STRING ,id STRING ,title STRING ,distance STRING\")\n",
    "df=df.withColumn('distance',lit(df.distance).cast(DoubleType()))\n",
    "#display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65ab98cf-e66e-40b7-b121-64e9022d19f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Returning best five search results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6de9400-1dfd-4cb9-86dd-d1ac84a0aed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>title</th><th>category</th></tr></thead><tbody><tr><td>Azure Database for PostgreSQL</td><td>Databases</td></tr><tr><td>Azure Cosmos DB</td><td>Databases</td></tr><tr><td>Azure SQL Database</td><td>Databases</td></tr><tr><td>Azure Database for MariaDB</td><td>Databases</td></tr><tr><td>Azure Database for MySQL</td><td>Databases</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Azure Database for PostgreSQL",
         "Databases"
        ],
        [
         "Azure Cosmos DB",
         "Databases"
        ],
        [
         "Azure SQL Database",
         "Databases"
        ],
        [
         "Azure Database for MariaDB",
         "Databases"
        ],
        [
         "Azure Database for MySQL",
         "Databases"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_result= df.select(df.title, df.category).sort(asc('distance')).limit(5)\n",
    "display(df_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c21eda1-e130-4a04-8fe7-c64c1153027a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Delete vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f35da9-6053-4e42-be69-fd7a6c637999",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vsc.delete_index(endpoint_name=vector_search_endpoint_name,index_name=vs_index_fullname)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks_vector_query",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
