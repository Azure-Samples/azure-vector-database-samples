{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Cognitive Search Vector Search via Python SDK\n",
    "This code demonstrates how to use Azure Cognitive Search with OpenAI and Azure Python SDK\n",
    "## Prerequisites\n",
    "To run the code, set up the conda environment using the environment.yml."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment variables\n",
    "\n",
    "\n",
    "Adjusting the file.\n",
    "vector extension works just before version 14. \n",
    "So I calculate vector using the Euclidian distance function from numpy. For now, I did a cross-join between the values on the same column of the file imported with the embeddings. \n",
    "the vector comparison between the table and the text still is not working. -> Similarity Vector Search with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install openai[datalib]\n",
    "!pip install python-dotenv\n",
    "!pip install azure-ai-textanalytics\n",
    "!pip install azure-search-documents --pre\n",
    "!pip install azure-search --pre --upgrade\n",
    "!pip install azure-core --pre --upgrade\n",
    "!pip install azure-storage-blob\n",
    "#!pip install azure-search-documents==11.4.0\n",
    "!pip install azure-identity\n",
    "!pip install azure-search-documents==11.4.0a20230509004 -i https://pkgs.dev.azure.com/azure-sdk/public/_packaging/azure-sdk-for-python/pypi/simple/ --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "\n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(key)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods\n",
    "Create your search index schema and vector search configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient, SearchIndexerClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ComplexField,\n",
    "    CorsOptions,\n",
    "    SearchIndex,\n",
    "    ScoringProfile,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField\n",
    ")\n",
    "##https://azuresdkdocs.blob.core.windows.net/$web/python/azure-search-documents/latest/index.html\n",
    "def get_index_client() -> SearchIndexClient:\n",
    "    return SearchIndexClient(service_endpoint, AzureKeyCredential(key))\n",
    "\n",
    "def create_index(index_name, fields, vector_search, semantic_title_field_name, semantic_content_field_names):\n",
    "    semantic_settings = SemanticSettings(\n",
    "        configurations=[SemanticConfiguration(\n",
    "            name='default',\n",
    "            prioritized_fields=PrioritizedFields(\n",
    "                title_field=SemanticField(field_name=semantic_title_field_name), prioritized_content_fields=[SemanticField(field_name=field_name) for field_name in semantic_content_field_names]))])\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        vector_search=vector_search,\n",
    "        semantic_settings=semantic_settings)\n",
    "    index_client = get_index_client()\n",
    "    return index_client.create_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('../data/pdf/employee_handbook_chunk_embeddings.json')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PostGres Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import re\n",
    "\n",
    "\n",
    "# Update connection string information\n",
    "postgree_params = {\n",
    "    \"host\": \"server.postgres.database.azure.com\",\n",
    "    \"port\": \"5432\", #postgres default\n",
    "    \"dbname\": \"postgres\",  #postgres default\n",
    "    \"user\": \"user name\",\n",
    "    \"password\": \" password\"\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**postgree_params)\n",
    "print(\"Connection established\")\n",
    "\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "# SQL query to fetch PostgreSQL version\n",
    "query = \"SELECT version();\"\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the result\n",
    "version_string = cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "# Use regular expression to extract the PostgreSQL version\n",
    "numeric_version_match = re.search(r'PostgreSQL (\\d+\\.\\d+)', version_string)\n",
    "if numeric_version_match:\n",
    "    Sversion = numeric_version_match.group(1)\n",
    "else:\n",
    "    Sversion = \"Version not found\"\n",
    "\n",
    "# Print the extracted PostgreSQL version\n",
    "print(f\"Numeric PostgreSQL Version: {Sversion}\")\n",
    "\n",
    "\n",
    "# Remove the period and convert to float\n",
    "version = float(Sversion.replace(\".\", \"\"))\n",
    "\n",
    "if version > 14:\n",
    "    print(\"Greater than 15 configuration, table column use Array\")\n",
    "    ##Postgree version<14 has no extension vector\n",
    "    table_schema_data = \"\"\"\n",
    "      id_serial UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n",
    "      chunk_content  text,\n",
    "      chunk_content_vector  double precision[]\n",
    "\"\"\"\n",
    "if version <= 14:\n",
    "      print(\"Smaller than 15 configuration, table column use Vector\")\n",
    "      #install pgvector -> need to add the extension at the database before create.\n",
    "      ##Postgree version<14\n",
    "      cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector\"); \n",
    "      conn.commit()\n",
    "      print(\"Adding extension - vector\")\n",
    "\n",
    "      # Define the table schema if needed\n",
    "      table_schema_data = \"\"\"\n",
    "            id_serial UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n",
    "            chunk_content text,\n",
    "            chunk_content_vector VECTOR(1536)\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "# Drop previous table of same name if one exists\n",
    "# Replace 'your_table_name' with the name of your PostgreSQL table\n",
    "table_name = \"chunk_content_embeddings\"\n",
    "table_schema = \"chunk\"\n",
    "\n",
    "cursor.execute(f\"CREATE schema IF NOT EXISTS {table_schema}\"); ##postgis\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Drop table\n",
    "cursor.execute(f\"DROP TABLE IF  EXISTS {table_schema}.{table_name} \")\n",
    "#Table Creatin\n",
    "cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_schema}.{table_name} ({table_schema_data});\")\n",
    "print(\"Drop the old table and Create a new(if old existed)\")\n",
    "\n",
    "\n",
    "\n",
    "# Clean up\n",
    "# Close the cursor and connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection Closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate table with Embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Update connection string information\n",
    "\n",
    "postgree_params = {\n",
    "    \"host\": \"server.postgres.database.azure.com\",\n",
    "    \"port\": \"5432\", #postgres default\n",
    "    \"dbname\": \"postgres\",  #postgres default\n",
    "    \"user\": \"user name\",\n",
    "    \"password\": \" password\"\n",
    "}\n",
    "\n",
    "table_name = \"chunk_content_embeddings\"\n",
    "table_schema = \"chunk\"\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have already defined your df DataFrame and postgree_params\n",
    "conn = psycopg2.connect(**postgree_params)\n",
    "print(\"Connection established\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "total_records = df.shape[0]\n",
    "\n",
    "# Iterate through your DataFrame and insert embeddings into PostgreSQL\n",
    "for index, row in df.iterrows():\n",
    "    insert_sql = f'''\n",
    "       INSERT INTO {table_schema}.{table_name} (chunk_content, chunk_content_vector)\n",
    "        VALUES (%s, ARRAY[%s]::double precision[]);\n",
    "    '''\n",
    "    # Use parameterized queries to safely insert data\n",
    "    cursor.execute(insert_sql, (row['chunk_content'], row['chunk_content_vector']))\n",
    "\n",
    "\n",
    "# Commit the changes and close the cursor and connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection Closed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Vector Search with the results\n",
    "##### Read table\n",
    "##### Create Dataframe\n",
    "##### Apply cosine function to get the similarities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from azure.search.documents.models import Vector  \n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Update connection string information\n",
    "postgree_params = {\n",
    "    \"host\": \"server.postgres.database.azure.com\",\n",
    "    \"port\": \"5432\", #postgres default\n",
    "    \"dbname\": \"postgres\",  #postgres default\n",
    "    \"user\": \"user name\",\n",
    "    \"password\": \" password\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**postgree_params)\n",
    "print(\"Connection established\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the query\n",
    "\n",
    "table_name = \"chunk_content_embeddings\"\n",
    "table_schema = \"chunk\"\n",
    "column_name_chunk = \"chunk_content_vector\"\n",
    "# Define the similarity threshold (adjust as needed)\n",
    "similarity_threshold = 0.5\n",
    "\n",
    "query_text = \"tools for software development\"\n",
    "\n",
    "\n",
    "query = f'''\n",
    "    SELECT t1.chunk_content_vector as chunk_content_vector,\n",
    "           t1.chunk_content as chunk_content\n",
    "    FROM {table_schema}.{table_name} t1\n",
    "    LIMIT 100;\n",
    "'''\n",
    "print(\"Query table\")\n",
    "\n",
    "cursor.execute(query)\n",
    "# Fetch and process the results\n",
    "query_results = cursor.fetchall()\n",
    "\n",
    "column_names = [desc[0] for desc in cursor.description]\n",
    "df_query_results = pd.DataFrame(query_results, columns=column_names)\n",
    "\n",
    "\n",
    "# search through the reviews for a specific product\n",
    "##Extract from: https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=command-line\n",
    "def search_docs(df, user_query,column_name_chunk, top_n=3, to_print=True):\n",
    "    embedding = get_embedding(\n",
    "        user_query,\n",
    "        engine=\"text-embedding-ada-002\" # engine should be set to the deployment name you chose when you deployed the text-embedding-ada-002 (Version 2) model\n",
    "    )\n",
    "    df[\"similarities\"] = df[column_name_chunk].apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = (\n",
    "        df.sort_values(\"similarities\", ascending=False)\n",
    "        .head(top_n)\n",
    "    )\n",
    "    if to_print:\n",
    "        display(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "res = search_docs(df_query_results, query_text,column_name_chunk, top_n=3)\n",
    "\n",
    "# Close the PostgreSQL connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
