{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Requisites\n",
    "#### Pip install before proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install psycopg2\n",
    "!pip install load_dotenv\n",
    "!pip install requests\n",
    "!pip install plotly\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "!pip install openai\n",
    "# The 'sklearn' PyPI package is deprecated, use 'scikit-learn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting environmnent tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from dotenv import load_dotenv  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "\n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "AZURE_OPENAI_EMBEDDING_DEPLOYMENT =  os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "##AZURE_OPENAI_EMBEDDING_DEPLOYMENT = 'text-embedding-ada-002'\n",
    "credential = AzureKeyCredential(str(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def insert_record(acs_endpoint, acs_index, data, acs_key, acs_api_version):\n",
    "    url = f\"{acs_endpoint}/indexes/{acs_index}/docs/index?api-version={acs_api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": acs_key\n",
    "    }    \n",
    "    response = requests.post(url, data=data, headers=headers)\n",
    "    print(response.status_code)\n",
    "    print(response.content)\n",
    "\n",
    "def create_index(acs_endpoint, json_content, acs_index, api_key, acs_api_version):\n",
    "    url = f\"{acs_endpoint}/indexes/{acs_index}?api-version={acs_api_version}\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": api_key\n",
    "    }\n",
    "    response = requests.request('PUT', url, headers=headers, data=json_content)\n",
    "    print(response.status_code)\n",
    "    print(response.content)\n",
    "\n",
    "def search_vector_similarity(query_vector, top_doc_count, acs_endpoint, acs_index,acs_key, acs_api_version):\n",
    "    url = f\"{acs_endpoint}/indexes/{acs_index}/docs/search?api-version={acs_api_version}\"\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": acs_key\n",
    "    }\n",
    "\n",
    "    request_body = {\n",
    "        \"vectors\": [{\n",
    "            \"value\": query_vector,\n",
    "            \"fields\": \"content_vector\",\n",
    "            \"k\": top_doc_count\n",
    "        }],\n",
    "        \"select\": \"title\"\n",
    "    }\n",
    "    request_body = json.dumps(request_body)\n",
    "\n",
    "    response = requests.request('POST', url, headers=headers, data=request_body)\n",
    "\n",
    "    docs = [(item['title']) for item in response.json()['value']]\n",
    "\n",
    "    return docs\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "Read your data, generate OpenAI embeddings \n",
    "Batch size serves to limit the dataframe size as also helps to manage the rate limit scenario for OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define your batch size (adjust as needed based on API rate limits)\n",
    "## This variable helps to limit the number of rows to added on the dataframe \n",
    "batch_size = 2\n",
    "\n",
    "# Read the entire JSON file into a DataFrame\n",
    "df = pd.read_json('../data/text/product_docs.json')\n",
    "\n",
    "# Slice the DataFrame to get the desired number of rows\n",
    "df = df.head(batch_size)\n",
    "\n",
    "\n",
    "# Apply get_embedding to your DataFrame\n",
    "df['title_vector'] = df['title'].apply(lambda x : get_embedding(x, engine = AZURE_OPENAI_EMBEDDING_DEPLOYMENT)) \n",
    "df['content_vector'] = df['content'].apply(lambda x : get_embedding(x, engine = AZURE_OPENAI_EMBEDDING_DEPLOYMENT)) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Table and Schema in Postgres\n",
    "##### Different versions maybe handled differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n",
      "Numeric PostgreSQL Version: 15.3\n",
      "Greater than 15 configuration, table column use Array\n",
      "Create Schema extension\n",
      "Drop the old table and Create a new(if old existed)\n",
      "Connection Closed\n"
     ]
    }
   ],
   "source": [
    "import psycopg2 #postgres conexion\n",
    "import re ##using this library to get the precise version of the database by searching the string version query results\n",
    "\n",
    "# Update connection string information\n",
    "postgree_params = {\n",
    "    \"host\": \"server.postgres.database.azure.com\",\n",
    "    \"port\": \"5432\", ##default 5432\n",
    "    \"dbname\": \"postgres\", ##default databas, change if needed\n",
    "    \"user\": \"user name\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**postgree_params)\n",
    "cursor = conn.cursor()\n",
    "print(\"Connection established\")\n",
    "\n",
    "\n",
    "# SQL query to fetch PostgreSQL version\n",
    "##version 14 and beyond has no vector extension, so the columns should be array.\n",
    "query = \"SELECT version();\"\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(query)\n",
    "\n",
    "# Fetch the result\n",
    "version_string = cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "# Use regular expression to extract the PostgreSQL version\n",
    "numeric_version_match = re.search(r'PostgreSQL (\\d+\\.\\d+)', version_string)\n",
    "if numeric_version_match:\n",
    "    Sversion = numeric_version_match.group(1)\n",
    "else:\n",
    "    Sversion = \"Version not found\"\n",
    "\n",
    "# Print the extracted PostgreSQL version\n",
    "print(f\"Numeric PostgreSQL Version: {Sversion}\")\n",
    "\n",
    "\n",
    "# Remove the period and convert to float\n",
    "version = float(Sversion.replace(\".\", \"\"))\n",
    "\n",
    "##Postgree version<14 has no extension vector\n",
    "if version > 14:\n",
    "    print(\"Greater than 15 configuration, table column use Array\")\n",
    "    table_schema_data = \"\"\"\n",
    "      id_serial UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n",
    "      title text,\n",
    "      content text,\n",
    "      title_vector double precision[],\n",
    "      content_vector double precision[]\n",
    "\"\"\"\n",
    "if version <= 14:\n",
    "      print(\"Smaller than 15 configuration, table column use Vector\")\n",
    "      cursor.execute(\"CREATE EXTENSION IF NOT EXISTS vector\"); \n",
    "      conn.commit()\n",
    "      print(\"Adding extension\")\n",
    "\n",
    "      # Define the table schema if needed\n",
    "      # Make sure it matches the structure of your DataFrame\n",
    "      table_schema_data = \"\"\"\n",
    "            id_serial UUID DEFAULT gen_random_uuid() PRIMARY KEY,\n",
    "            title text,\n",
    "            content text,\n",
    "            title_vector VECTOR(1536),\n",
    "            content_vector VECTOR(1536)\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "# Drop previous table of same name if one exists\n",
    "# Replace 'vctor_embeddings' with the name of your PostgreSQL table\n",
    "table_name = \"vctor_embeddings\"\n",
    "# Replace 'vctor' schema with the schema name of your PostgreSQL table\n",
    "table_schema = \"vctor\"\n",
    "\n",
    "\n",
    "#create schema\n",
    "cursor.execute(f\"CREATE schema IF NOT EXISTS {table_schema}}\"); \n",
    "conn.commit()\n",
    "print(\"Create Schema extension\")\n",
    "\n",
    "\n",
    "#Drop table\n",
    "cursor.execute(f\"DROP TABLE IF  EXISTS {table_schema}.{table_name} \")\n",
    "#Table Creatin\n",
    "cursor.execute(f\"CREATE TABLE IF NOT EXISTS {table_schema}.{table_name} ({table_schema_data});\")\n",
    "print(\"Drop the old table and Create a new(if old existed)\")\n",
    "\n",
    "\n",
    "# Clean up\n",
    "# Close the cursor and connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection Closed\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting Data into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established\n",
      "Inserted 3 records into PostgreSQL\n",
      "Finished inserting 3 records into PostgreSQL\n",
      "Connection Closed\n"
     ]
    }
   ],
   "source": [
    "#import psycopg2\n",
    "\n",
    "# Update connection string information\n",
    "\n",
    "postgree_params = {\n",
    "    \"host\": \"server.postgres.database.azure.com\",\n",
    "    \"port\": \"5432\", ##default 5432\n",
    "    \"dbname\": \"postgres\", ##default databas, change if needed\n",
    "    \"user\": \"user name\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "# Replace 'vctor_embeddings' with the name of your PostgreSQL table\n",
    "table_name = \"vctor_embeddings\"\n",
    "# Replace 'vctor' schema with the schema name of your PostgreSQL table\n",
    "table_schema = \"vctor\"\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**postgree_params)\n",
    "print(\"Connection established\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "total_records = df.shape[0]\n",
    "\n",
    "# Iterate through your DataFrame and insert embeddings into PostgreSQL\n",
    "for index, row in df.iterrows():\n",
    "    insert_sql = f'''\n",
    "       INSERT INTO {table_schema}.{table_name} (title,content,title_vector, content_vector)\n",
    "        VALUES ('{row['title']}', '{row['content']}',\n",
    "                ARRAY[{','.join(map(str, row['title_vector']))}]::double precision[],\n",
    "                ARRAY[{','.join(map(str, row['content_vector']))}]::double precision[]);\n",
    "    '''\n",
    "    cursor.execute(insert_sql)\n",
    "\n",
    "\n",
    "\n",
    "if index % batch_size == 0 or (index + 1 == total_records):\n",
    "    print(f\"Inserted {index+1} records into PostgreSQL\")\n",
    "\n",
    "print(f\"Finished inserting {total_records} records into PostgreSQL\")\n",
    "\n",
    "# Close the database connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Query Data at Postgres and checking results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Checking if the data was inserted sucessfully\n",
    "#import psycopg2\n",
    "\n",
    "# Update connection string information\n",
    "postgree_params = {\n",
    "    \"host\": \"server.postgres.database.azure.com\",\n",
    "    \"port\": \"5432\", ##default 5432\n",
    "    \"dbname\": \"postgres\", ##default databas, change if needed\n",
    "    \"user\": \"user name\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**postgree_params)\n",
    "print(\"Connection established\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "table_name = \"vctor_embeddings\"\n",
    "table_schema = \"vctor\"\n",
    "\n",
    "\n",
    "# Define the SELECT query\n",
    "select_query = f'SELECT * FROM {table_schema}.{table_name} ;'\n",
    "\n",
    "# Execute the query\n",
    "cursor.execute(select_query)\n",
    "\n",
    "# Fetch all rows\n",
    "rows = cursor.fetchall()\n",
    "\n",
    "# Process the results\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "\n",
    "# Close the database connection\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"Connection Closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectordb2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
